---
layout: post
title: Notes on Regularized Bayesian Inference
categories:
- Academic
tags:
- Bayesian nonparametrics
---

## *Notes on Regularized Bayesian Inference* 

-------
Bayesian Nonparametrics
-------

### Dirichlet Process
> - **Chinese Restaurant Process**
> - **Stick-Breaking Construction of DP**
> - **Reference**


$\mathcal{Prerequisite:}\ $ Basic measure theory

***Definition***: Let $G_0$ be a distribution over the sample space $ \Theta $ and $\alpha$ a positive number, any finite measurable partition $(A_1, A_2, ..., A_m)$ of $ \Theta $ follows Dirichlet distribution:
$$
(G(A_1), ..., G(A_m)) \sim Dir(\alpha G_0(A_1), ..., \alpha G_0(A_m))
$$

We then say $ G $ is Dirichlet process distributed with base distribution $G_0$ and concentration parameter $\alpha$, written: $ G \sim DP(\alpha, G_0) $


***Posterior distribution*** of G given observed values of $\theta_1, \theta_2, ..., \theta_n$:  
$$ G | \theta_1, ..., \theta_n \sim DP(\alpha + n,\  \frac{\alpha}{\alpha+n} G_0 + \frac{1} {\alpha+n}  \sum_{i=1}^{n} \delta_{\phi_i}) $$

***Predictive distribution*** (with $G$ marginalized out):
$$
\theta_{n+1} | \theta_1, ..., \theta_{n} \sim \frac{1}{\alpha + n} (\alpha G_0 + \sum_{i=1}^{n} \delta_{\phi_i} )
$$

$\mathcal{TODO:}\ $ 
Theoretical connection between *Dirichlet process* and *Poisson process* / *Spatial Poisson process*.

#### Chinese Restaurant Process
Integrate out G, we obtain a distribution over partitions called the Chinese Restaurant process (analogous to Polya's urn scheme)

CRP is a random process in which n customers sit down in a Chinese restaurant with an infinite number of tables:

- the first customer sits at the first table
- the $n_{th}$ customer chooses a table with probability:
$$
P(C_n = c|C_1,\ldots,C_{n-1})
 = \begin{cases}
\dfrac{\alpha}{\alpha + n - 1} & \text{if }c \in \text{new table}, \\  \\
\dfrac{n_k}{\alpha + n - 1} & \text{if }c\in \text{table k, } \small{k \in {1, ..., n-1}};
\end{cases}
$$

**Relation with DP:**
The random partition is *exchangeable* in the sense that relabeling ${1, ..., n}$ does not change the distribution of the partition.

*De Finetti’s theorem* states that for any infinitely exchangeable sequence $\theta_1, \theta_2, ..., \theta_n$, there is a random distribution G such that the sequence is composed of i.i.d. draws from it, and $p(G)$ is the *de Finetti mixing distribution*:
$$ 
p(\theta_1, ..., \theta_n ) = \int \prod_{i=1}^nG(\theta_i)\ dP(G) 
$$ 

The Dirichlet process is the *de Finetti mixing measure* for the *Chinese Restaurant Process*, meaning that sampling i.i.d. from a draw of a DP is equivalent to sequentially drawing samples from the CRP.

$\mathcal{TODO:}\ $ 

- Proof of the de Finetti's theorem.
- Pitman--Yor process

#### Stick-Breaking Construction of DP
G can be represented explicitly as an infinite weighted sum of atomic measures, and Stick-breaking process is a constructive approach for generating a Dirichlet process:

$$
\begin{aligned}
    & \beta_k \sim Beta(1, \alpha) 
        \ \  \quad  \quad  \quad \theta_k \sim G_0 \\
    & \pi_k = \beta_k\prod_{i=1}^{k-1}(1-\beta_i)                  
        \quad  \quad   G = \sum_{k=1}^{\infty}\pi_k \delta_{\theta_k}  
\end{aligned} 
$$

#### Reference
> - Y. W. Teh. [Dirichlet Process][1]
> - B. A. Frigyik, A. Kapila, and M. R. Gupta. [Introduction to the Dirichlet Distribution and Related Processes][2]

### Dirichlet Process Mixtures and Inference
> - **MCMC Sampling for DPM**
> - **Variational Inference for DPM**
> - **Reference**


Draws from a Dirichlet process can be represented as an infinite mixture model:
$$G := \sum_{k=1}^{\infty}\pi_k \delta_{\phi_k} \sim \mathcal{DP}(\alpha, G_0)$$

Use the stick-breaking construction, the data can be described as arising from the following process:

1. Draw $v_i| α ∼ Beta(1, α),\ i = {1,2, . . .}$
2. Draw $η^∗_i| G_0 ∼ G_0,\ i = {1,2, . . .}$
3. For the $n_{th}$ data point:
    a) Draw $z_n |{v_1,v_2, . . .} ∼ Mult(π(v))$.
    b) Draw $x_n |z_n ∼ p(x_n |η^∗_{z_n})$.


#### MCMC Sampling for DPM
$\mathcal{Prerequisite:}\ $ Markov chain Monte Carlo, EM algorithm

**Collapsed Gibbs sampler**: 
The collapsed Gibbs sampler for a DP mixture with conjugate base distribution integrates out the random measure G and distinct parameter values ${η^∗_1, ..., η^∗_{|c|}}$, the Markov chain is thus defined only on the latent partition $c = {c_1, . . . , c_N}$

The algorithm iteratively samples each assignment variable $c_n$, for $n ∈ {1, . . . , N}$, conditional on the other cells in the partition, $\mathbf{c}_{−n}$. The assignment $c_n$ can be in a cell either with other data points or by itself.

*Exchangeability* implies that $c_n$ has the following multinomial distribution:
$$
p(c_n = k | \mathbf{x}, \mathbf{c}_{−n}, α, λ) ∝ p(x_n | \mathbf{x}_{−n}, \mathbf{c}_{−n}, c_n = k, λ)\cdot p(c_n = k | \mathbf{c}_{−n}, α).
$$

Once this chain has reached its stationary distribution, we collect B samples ${c_1, . . . , c_B}$ to approximate the *posterior*. The *approximate predictive distribution* is an average of the predictive distributions across the Monte Carlo samples:
$$
p(x_{N+1} | x_1, ..., x_N, α, λ) = \frac{1}{B} \prod^{B}_{b=1} p(x_{N+1} | \mathbf{c}_b, \mathbf{x}, α, λ).
$$

When $G_0$ is not conjugate, the distribution in  does not have a simple closed form, then we can resort to other methods, e.g. Gibbs sampling with auxiliary parameters.

In the collapsed Gibbs sampler, a new sample of assignment variable $c_n$ is dependent on the most recently sampled values of the other assignment variables, thus these variables must be updated one at a time which can be rather inefficient and hard to parallelize.

**Blocked Gibbs sampler**: 

Define a truncated Dirichlet process which can closely approximates a full Dirichlet process when the truncation level is chosen large relative to the number of data points.
$$G^K = \sum_{k=1}^{K}\pi_k \delta_{\theta_k}  $$

In the TDP mixture, the state of the Markov chain consists of the beta variables $v = {v_1, ..., v_{K−1}}$, the mixture component parameters $\mathbf{η}^∗ = {η^∗_1, . . . ,η^∗_K}$, and the indicator variables $\mathbf{z} = {z_1, . . . ,z_N}$. The blocked Gibbs sampler iterates between the following three steps:

 1. For $n ∈ {1, ..., N}$, independently sample $z_n$ from
$$
p(z_n = k | \mathbf{v}, \mathbf{η}^∗, \mathbf{x}) = π_k(\mathbf{v})p(x_n |η^{∗}_k)
$$

 2. For $k ∈ {1, ..., K}$, independently sample $v_k$ from
$$v_k|rest ∼ Beta(1 + m_k, α+\sum^{K}_{j=k+1}m_j)$$

 3. For $k ∈ {1, . . . , K}$, independently sample $η^∗_k$ from $p(η^∗_k |τ_k)$. This distribution is in the same family as the base distribution.

After the chain has reached its stationary distribution, we collect B samples and construct an *approximate predictive distribution*. The predictive distribution for a particular sample is
$$
p(x_{N+1} | \mathbf{z}, \mathbf{x}, α, λ) = \sum^{K}_{k=1} E[π_i(\mathbf{v})| γ_1, . . . , γ_k]\cdot p(x_{N+1} | τ_k)
$$

$\mathcal{TODO:}$
Distributed MCMC for DPMs

#### Variational Inference for DPM
$\mathcal{Prerequisite:}\ $ Variational inference

Variational inference is based on reformulating the problem of computing the posterior distribution as an optimization problem, perturbing that problem, and finding solutions to the relaxed problem.

*Mean-field* variational method is based on optimizing Kullback-Leibler (KL) divergence with respect to a variational distribution. In particular, let $q_ν(\mathbf{w})$ be a family of distributions indexed by a variational parameter $ν$. We aim to minimize the KL divergence between $q_ν(\mathbf{w})$ and $p\mathbf{(w} | \mathbf{x}, θ)$.

$$
KL(q_ν(\mathbf{w})||p(\mathbf{w}| \mathbf{x},θ)) = E_q[log\ q_ν(\mathbf{w})] - E_q[log\ p(\mathbf{w}, \mathbf{x} |θ)] + log\ p(\mathbf{x}|θ)
$$

In constructing the tractable family of variational distribution $q_v(\mathbf{w})$, one typically breaks some of the dependencies between latent variables that make the true posterior difficult to compute.

Variational bound on the log marginal probability of the data:
$$
\begin{aligned}
log\ p(\mathbf{x} | α, λ) ≥ 
& E_q [log\ p(\mathbf{V} | α)] + E_q [log\ p(\mathbf{η}^∗| λ)] \\
& + \sum ^N_{n=1}(E_q [log\ p(Z_n | \mathbf{V})] + E_q [log\ p(x_n |Z_n)]) \\
& −\ E_q [log\ q(\mathbf{V}, \mathbf{η}^∗, \mathbf{Z})] 
\end{aligned}
$$

To exploit this bound, we consider the truncated stick-breaking representation as the approximation of the full Dirichlet process.

Proposed fully-factorized family of variational distributions:
$$q(\mathbf{v}, \mathbf{η}^∗, \mathbf{z}) = \prod^{T−1}_{t=1}q_{γ_t}(v_t)\prod^{T}_{t=1}q_{τ_t}(η^∗_t)\prod^{N}_{n=1}q_{φ_n}(z_n)$$

where $q_{γ_t}(v_t)$ are beta distributions, $q_{τ_t}(η^∗_t)$ are exponential family distributions with natural parameters $τ_t$, and $q_{φ_n}(z_n)$ are multinomial distributions.

Use *coordinate ascent algorithm* for optimizing the lower bound with respect to the variational parameters.

Approximate the predictive distribution with a product of expectations over the variational posterior:
$$
p(x_{N+1} | \mathbf{x}, α, λ) ≈ \sum^T_{t=1} E_q [π_t(\mathbf{V})]\cdot E_q [p(x_{N+1} |η^∗_t)] 
$$

$\mathcal{TODO:}$
Collapsed variational inference for DPMs

#### Reference
> - R. Neal. [Markov Chain Sampling Methods for Dirichlet Process Mixture Models][3]
> - D. Blei and M. Jordan. [Variational Inference for Dirichlet Process Mixtures][4]


### Hierarchical Dirichlet Processes
> - **Chinese Restaurant Franchise**
> - **Infinite Hidden Markov Models**
> - **Reference**

Hierarchical Dirichlet Processes are applied to clustering grouped data. It uses a Dirichlet process for each group of data, with the DPs all sharing a base measure which is itself drawn from a Dirichlet process.
$$
 G_0 \sim DP(\gamma, H) \\
 G_i \sim DP(\alpha, G_0) \\
 \theta_{ij} \sim G_i \\
 x_{ij}|\theta_{ij} \sim p(x_{ij}|\theta_{ij})
$$

#### Chinese Restaurant Franchise

By adding one more level of Dirichlet Process over the base measure $G_0$, HDP enables data in groups to share countable infinite cluster identities and to exhibit unique cluster propositions.

Like Chinese Restaurant process being an intuitive interpretation of Dirichlet process, i.e. an induced distribution over partitions, Chinese Restaurant franchise is an extended analog of CRP for HDP.

Suppose a franchise of restaurants, sharing an countably infinite menu, a customer enters one of the restaurants and choose a table using the same scheme as CRP, and with it a dish from the global menu. Each table in each restaurant picks a dish, with probability proportional to the number of times it has been served across all restaurants.


#### Infinite Hidden Markov Model
$\mathcal{Prerequisite:}\ $ Hidden Markov Model

An HMM has K hidden states and a K*K transition matrix, thus the number of hidden states for infinite HMM, by extension, is  *unbounded* / *countably infinite*.

For HMM, the number of hidden state is finite(K), we can think the transition matrix as a collection of multinomial distribution, and each row in the matrix is a multinomial distribution that tells us where would the state at current time transit to.

As each row becoming infinite, it can be drawn from a Dirichlet process, instead of a multinomial distribution. Since an HDP consists of infinite number of Dirichlet processes with a shared base measure, naturally it can be used as prior for the transition matrix of iHMM.

HDP couples transitions between different states, thus the rows of transition matrix are coupled to allow dependencies which is essential to provide non-trivial solutions through inference when number of hidden states goes to infinity.

Clearly it is unreasonable to infer the infinitely many parameters in the transition matrix; instead by using the theory of Dirichlet processes, we can implicitly integrate them out, leaving only three hyper-parameters defining the prior over transition dynamics.


#### Reference
> - Y. Teh, M. Jordan, M. Beal, and D. Blei. [Hierarchical Dirichlet Processes][5]
> - M. Beal, Z. Ghahramani, and C. Rasmussen. [Infinite Hidden Markov Model][6]

### The Indian Buffet Process
> - **Infinite Latent Feature Models and the Indian Buffet Process**
> - **Relation to Beta Process**
> - **Stick-breaking Construction for IBP**
> - **Reference**

#### Infinite Latent Feature Models and IBP

Binary feature matrix $Z$ of *Finite* latent feature model:
$$
\pi_k \sim  Beta(\alpha/K, 1),\ k = 1, ..., K \\
z_{nk} \sim Bernoulli(\pi_k),\ n = 1, ..., N
$$

Define the left-order-function $lof(Z)$ to be the matrix that reorders columns according to their magnitude as binary numbers.

Generative procedure of IBP$:$
Suppose a restaurant with an infinitely large buffet:

- The first customer helps him/herself to the first $K_1 \sim Poisson(\alpha)$ dishes;
- the $n_{th}$ customer:
    - chooses each dish with probability $\frac{m_k}{n}$, where $m_k$ is the number of times dish k was chosen before;
    
    - then tries $K_n \sim Poisson(\frac{\alpha}{n})$ new dishes.

#### Relation to Beta Process
Beta process is the *de Finetti mixing distribution* underlying the Indian buffet process, as Dirichlet process to Chinese restaurant process.
$$
B ∼ \mathcal{BP}(c, B_0) \\
Z ∼ \mathcal{BeP}(B)
$$
#### Stick-breaking construction for IBP
Let $\pi_{(1)}, \pi_{(2)}, ..., \pi_{(K)}$ be a decreasing ordering of $\pi_{1:K} = {\pi_1, . . . , \pi_K}$,
$$
\begin{aligned}
& ν_{(k)} \sim Beta(α,1) \\
& \pi_{(1)} = ν_{(1)}, \quad \pi_{(k)} = ν_{(k)}\pi_{(k−1)} = \prod^{k}_{i=1} ν_{(i)}
\end{aligned}
$$

$\mathcal{TODO:}$

- (Hierarchical) Beta Process
- Relation to Dirichlet process
- Inference of IBP

#### Reference
> - T. Griffiths, Z. Ghahramani. [Infinite Latent Feature Models and the Indian Buffet Process][7]
> - T. Griffiths, Z. Ghahramani. [The Indian Buffet Process: An Introduction and Review][8]
> - R. Thibaux and M. I. Jordan. [Hierarchical Beta Processes and the Indian Buffet Process][9]
> - Y. W. Teh, D. Gorur, and Z. Ghahramani. [Stick-breaking Construction for the Indian Buffet Process][10]

------
Maximum-Margin Learning of Graphical Models
------

### Maximum-Margin Markov Networks
> - **Support Vector Machines**
> - **Conditional Random Fields**
> - **Max-margin Markov Networks**
> - **Reference**

#### Support Vector Machines
$\mathcal{Prerequisite:}\ $ Linear models for classification, Convex optimization (*duality*)

- Max-margin classifer (hinge loss)
- Lagrangian duality
- Sparsity (support vectors)
- Kernel methods (handle high-dimensional feature space)
- Generalization guarantees (VC dimension)

#### Conditional Random Fields
$\mathcal{Prerequisite:}\ $ Markov random fields

- Discriminative(task-specific) graphical model
- Have rich representations (global/joint feature), good performance on structured data with correlated features

#### Max-margin Markov Networks

- Integrate generative models (Markov network) and large margin approach (SVM) to achieve structured prediction.


**Primal:** 
$$
\begin{aligned} 
& min  \  \frac {1}{2} \|w\|^2 + C\sum_x\xi_x  \\
& s.t. \ \ w^T \triangle f_x(y) \ge \triangle t_x(y)- \xi_x,\ \forall x, y
\end{aligned}
$$

**Dual:** 
$$
\begin{aligned} 
& max  \ \sum_{x,y}\alpha_x(y)\ \triangle t_x(y) - \frac {1}{2} \|\sum_{x,y}\alpha_x(y)\ \triangle f_x(y)\|^2  \\
& \begin{aligned} 
  s.t.\ 
        & \sum_y\alpha_x(y) = C,\ \forall x \\
        & \ \alpha_x(y) \ge 0,\ \forall x, y
        \end{aligned}
\end{aligned}
$$

**Insights:**

- Generalized notion of margin for multi-label classification: margin  should be proportional to the number of single-label errors;
- Utilize sparse correlations in feature representation to obtain factorized dual;
- Dual variables will factorize the same way as feature functions;
- Define consistency conditions to ensure that the transformed dual problem is equivalent to the original dual QP problem.

#### Reference
> - B. Taskar, C. Guestrin, and D. Koller. [Max-Margin Markov Networks][11]
> - S. Lacoste-Julien. [An Introduction to Max-Margin Markov Networks][12]


### Maximum Entropy Discrimination Markov Networks
> - **Maximum Entropy Discrimination**
> - **MaxEnDNets**
> - **Reference**

#### Maximum Entropy Discrimination
MED is a general framework for discriminative estimation based on maximum entropy principle, it combines the strength of both likelihood-based estimation and max-margin learning by introducing probabilistic interpretation into SVM.

Instead of finding the weight vector $\mathbf{w}$ for classification by point estimate, MED learns the distribution of model parameters $p(\mathbf{w})$ based on *minimum relative entropy principle.*

$$ 
\begin{aligned} 
& \inf_{p(\mathbf{w})} \ \ \ KL(p(\mathbf{w})\|p_0(\mathbf{w}))  \\
& \ s.t.: \int p(\mathbf{w}) [y_i F_i(\mathbf{x}; \mathbf{w}) - \xi_i] d\mathbf{w} \ge 0,\ \forall i
\end{aligned}
$$

Prediction can be performed through *model averaging*:
$$
\mathbf{y} = \text{sign} \int p(\mathbf{w})F(\mathbf{x}; \mathbf{w})d\mathbf{w} 
$$

#### MaxEnDNet
MaxEnDNet is essentially a structured version of MED built on max-margin Markov networks.



$$ 
\begin{aligned} 
& \inf_{p(\mathbf{w}),\ \boldsymbol{\xi} } \ KL(p(\mathbf{w})\|p_0(\mathbf{w})) + U(\boldsymbol{\xi}) \\
& \ s.t.:\ p(\mathbf{w}) \in \mathcal{F}_{1},\ \xi_i \ge 0, \forall i
\end{aligned}
$$

Define generalized entropy (regularized KL-divergence) to accommodate non-separable cases, $U(\boldsymbol{\xi})$ is the convex function over slack variables: 
$$KL(p(\mathbf{w})\|p_0(\mathbf{w}))$$

Use *expected margin constraint*: probabilistic version of M3N's margin constraints:
$$
\mathcal{F}_{1} = \left\{ p(\mathbf{w}): \int p(\mathbf{w}) [\triangle F_i(\mathbf{y}; \mathbf{w}) - \triangle \mathcal{t}_i(\mathbf{y})] d\mathbf{w} \ge - \xi_i,\ \forall i, \forall \mathbf{y} \ne \mathbf{y}^i \right\}
$$

Closed-form solution to the posterior distribution ($α_i(\mathbf{y})$ are the Lagrangian multipliers corresponding to the constraints in $\mathcal{F}_1$):
$$
p(\mathbf{w}) = \frac{1}{Z(\alpha)} p_0(\mathbf{w})\ exp \left\{ \sum_{i, \mathbf{y} \ne \mathbf{y}^i} \alpha_i(\mathbf{y})[\triangle F_i(\mathbf{y}; \mathbf{w}) - \triangle \mathcal{t}_i(\mathbf{y})] \right\}
$$

Three major advantages:

- MaxEnDNet is an averaging model, with *PAC-Bayes prediction error guarantee*
- *Entropy regularization* to introduce useful biases
- Combine *generative* and *discriminative* principles

Instantiations:

- *Gaussian* MaxEnDNet (can reduce to $M^3N$) 
- *Laplace* MaxEnDNet (a sparse $M^3N$, primal and dual sparsity)
- *Partially observed* MaxEnDNet

#### Reference
> - T. Jaakkola, M. Meila, and T. Jebara. [Maximum Entropy Discrimination][13]
> - J. Zhu and E. Xing. [Maximum Entropy Discrimination Markov Networks][14]


### Max-Margin Supervised Topic Models
> - **Latent Dirichlet allocation**
> - **Supervised Topic Models**
> - **MedLDA**
> - **Gibbs MedLDA**
> - **Reference**

#### Latent Dirichlet allocation
$\mathcal{Prerequisite:}\ $ Probabilistic latent semantic analysis (*pLSA*)

Bayesian version of *pLSA* (introduce Dirichlet distribution as conjugate prior on topics):

- A Bayesian mixture model with topical bases
- Each document is a mixture model over topics; each word is generated by one topic (distribution over words/corpus)

The generative process:
 
- for each document d, sample a topic proportion $\theta_d \sim Dir(\alpha)$;
- for each word/term, sample the topic assignment $z_{d, n} \sim Mult(\theta_d)$, and sample the word $w_{d, n} \sim Mult(\beta_{z_{d, n}})$

The joint distribution for LDA:
$$
p(\boldsymbol{\theta}, \mathbf{z}, \mathbf{w} | \alpha, \beta) = \prod_{d=1}^{D} p(\theta_d|\alpha) \left( \prod_{n=1}^{N} p(z_{d,n}|\theta_d) p(w_{d,n}|z_{d,n}, \beta) \right)
$$

**Inference:** 

- Collapsed Gibbs sampling
- Variational mean-field algorithm
- Collapsed variational inference

#### Supervised Topic Models

Supervised LDA adds a response variable to LDA associated with each document to capture the side information of data, e.g. use ratings for documents as response variable.

sLDA model is trained by maximizing the *joint* likelihood of the content data (e.g., text or image) and the responses (e.g., labeling or rating), thus the model finds latent topics that would best predict the response variables for future content data.

Model parameters: the K topics $β_{1:K}$ (each $β_k$ a vector of term probabilities), the Dirichlet parameter $α$, and the response parameters $η, σ^2$. Under the sLDA model, each document and response arises from the following generative process:

1. Draw topic proportions $θ | α ∼ Dir(α)$.
2. For each word
a) Draw topic assignment $z_n | θ ∼ Mult(θ)$.
b) Draw word $w_n |z_n, β_{1:K} ∼ Mult(β_{z_n})$.
3. Draw response variable $y |z_{1:N} , η, σ^2 ∼ N(η^T\overline{z}, σ^2)$
 

#### MedLDA

MedLDA integrates the mechanism behind max-margin learning (e.g., SVMs) and hierarchical Bayesian topic models (e.g., LDA) by optimizing a single objective function with expected margin constraints. 

It employs a composite objective motivated by a trade-off between two components—the negative log-likelihood of an underlying topic model which measures the goodness of fit for document contents, and a measure of response prediction error on training data. It then seeks a regularized posterior distribution of the predictive function in a feasible space defined by a set of expected margin constraints.

**MedTM**: the general framework

$$  \begin{aligned}
        & \min_{q(H),\ q(\eta),\ \Psi,\ \xi}\ 
            \mathcal{L}(q(H)) + KL(q(\eta) \| p_0(\eta)) + U(\boldsymbol{\xi}) \\ 
        & \quad\ \ s.t. \quad\quad expected\ margin\ constraints
    \end{aligned}
$$

$\mathcal{L}$ is the *variational upper bound* of the negative log likelihood associated with the underlying topic model;
$H$ are the *hidden variables*;
$U$ is the convex function over *slack variables*, accountable for the predictive accuracy.

Inference can be performed through variational EM algorithm.

Prediction using averaging model: $ \hat{\mathbf{y}} = \text{sign}\ E_{q}[η^T\overline{\mathbf{z}}]$

#### Gibbs MedLDA

MedLDA chooses the strategy to minimize the *hinge loss* of an expected classifier,  Gibbs MedLDA minimize an *expected margin loss*, under the framework known as Gibbs classifiers; Gibbs MedLDA can be seen as a relaxation of MedLDA.

The expected margin loss is defined as follows. If we have drawn a sample of the topic assignments $\mathbf{z}$ and the prediction model $η$ from a posterior distribution $q(η, \mathbf{z} | \mathbf{y}, \mathbf{w})$, we can define the linear discriminant function $f(η, \mathbf{z}; \mathbf{w}) = η^⊤\overline{\mathbf{z}}$ as before and make prediction using the Gibbs classifier: $\hat{\mathbf{y}} = \text{sign}\ f(η, \mathbf{z}; \mathbf{w}),\ η, \mathbf{z} \sim q(η, \mathbf{z} | \mathbf{y}, \mathbf{w})$


**RegBayes problem formulation:** (Bayesian inference with max-margin posterior constraints)
$$
\min_{q(η, Θ, \mathbf{Z}, Φ)\in \mathcal{P}} \mathcal{L}(q(η, Θ, \mathbf{Z}, Φ)) + 2c\cdot\mathcal{R}'(q)
$$

$\mathcal{R}'(q)$ (*expected hinge loss*) is an upper bound of the *hinge loss* $\mathcal{R}(\eta; Z)$ and the empirical risk (*expected training error*), thus can be a good surrogate loss for learning the posterior distribution:

$$
\begin{aligned}
\mathcal{R}'(q) 
& = E_q[\mathcal{R}(\eta; Z)] = \sum^{D}_{d=1} E_q[max(0, 1−y_d f(\mathbf{w}_d))] \\
& \ge \mathcal{R}(\eta; Z) = \sum^{D}_{d=1} max(0, 1-y_d f(\mathbf{w}_d))
\end{aligned}
$$

$\text{Expected training error of the Gibbs classifier:}$
$$
\hat{\mathcal{R}}(\eta; Z) = \sum^{D}_{d=1} E_q[[\mathbb{I}(\hat{y}_d \ne y_d)]
$$

**Problem formulation with Data Augmentation:**

Let $ϕ(y_d|\mathbf{z}_d, \mathbf{η}) = exp\left\{−2c\cdot\max(0, ζ_d)\right\}$ be the unnormalized pseudo-likelihood of the response variable for document d (reformulate the expected hinge loss):
$$
\min_{q(\mathbf{η}, \mathbf{Θ}, \mathbf{Z}, \mathbf{Φ})\in \mathcal{P}} \mathcal{L}(q(\mathbf{η}, \mathbf{Θ}, \mathbf{Z}, \mathbf{Φ})) + E_q[log\ ϕ(\mathbf{y}|\mathbf{Z}, \mathbf{η})]
$$

The posterior distribution of Gibbs MedLDA, $q(\mathbf{η}, \mathbf{Θ}, \mathbf{Z}, \mathbf{Φ})$, can be expressed as the marginal of a higher dimensional distribution that includes the augmented variables $\mathbf{λ}$.

The mixing rate of Gibbs sampler/Markov chain would be slow due to the large sample space of $q(\mathbf{η}, \mathbf{λ}, \mathbf{Θ}, \mathbf{Z}, \mathbf{Φ})$. One way to effectively reduce the sample space and improve mixing rates is to integrate out the intermediate Dirichlet variables $(Θ, Φ)$ and build a Markov chain whose equilibrium distribution is the resulting marginal distribution $q(η, λ, Z)$, we can then develop a collapsed Gibbs sampler using it as the collapsed posterior distribution.

Thus by introducing augmented variables and integrating out the Dirichlet variables analytically by conjugacy, a simple "*augment-and-collapse*" *Gibbs sampling* algorithm can be developed with no restricting assumptions on the posterior distributions and no need to solve SVM subproblems. 


#### Reference
> - D. M. Blei. [Introduction to Probabilistic Topic Models][15]
> - T. Griffiths and M. Steyvers. [Finding Scientific Topics][16]
> - D. M. Blei, J. D. McAuliffe. [Supervised Topic Models][18]
> - J. Zhu, A. Ahmed, and E. P. Xing. [MedLDA: Maximum Margin Supervised Topic Models for Regression and Classification][19]
> - J. Zhu, A. Ahmed, and E. P. Xing. [MedLDA: Maximum Margin Supervised Topic Models][21]
> - J. Zhu, N. Chen, H. Perkins, B. Zhang. [Gibbs Max-margin Topic Models with Fast Sampling Algorithms][22]
> - J. Zhu, N. Chen, H. Perkins, B. Zhang. [Gibbs Max-margin Topic Models with Data Augmentation][23]


------
Posterior Regularization on Nonparametric Models
------

> - **RegBayes framework**
> - **Infinite SVMs**
> - **Infinite Latent SVMs**
> - **Reference**

### RegBayes framework

- A general framework to integrate Bayesian nonparametrics and Max-margin learning.
- Constraints directly induced on posterior can encode rich structures/domain knowledge.
- Can be effectively solved by convex duality theory.

**Insights:**

- Bayes’ theorem can be reformulated as a minimization problem, consider the Bayesian posterior distribution as the solution to a minimization rule.

$$
\mathbf{RegBayes}:
\inf_{q(\mathbf{M|\mathcal{D}}) \in \mathcal{P}_{prob}} \mathcal{L}(q(\mathbf{M|\mathcal{D}})) + \Omega(q(\mathbf{M|\mathcal{D}}))
$$

With additional knowledge/data-driven constraints or regularization imposed on $q$:
$$ 
\begin{aligned} 
& \inf_{q(\mathbf{M}),\ \boldsymbol{\xi} } \ KL(q(\mathbf{M})\|\pi(\mathbf{M}))- \int_\mathcal{M} log \ p(\mathcal{D}|\mathbf{M})\ q(\mathbf{M})d\mathbf{M} + U(\boldsymbol{\xi}) \\
& \ s.t.:\ q(\mathbf{M}) \in \mathcal{P}_{post} (\boldsymbol{\xi})
\end{aligned}
$$

The problem can be rewritten as:
$
\begin{aligned}
\mathcal{L}(q(\mathbf{M})) 
& = KL(q(\mathbf{M})\|\pi(\mathbf{M}))- \int_\mathcal{M} log \ p(\mathcal{D}|\mathbf{M})\ q(\mathbf{M})d\mathbf{M} \\
& = KL(q(\mathbf{M})\|\pi(\mathbf{M, \mathcal{D}}))
\end{aligned}
$

$
\Omega(q(\mathbf{M})) = \inf_{\boldsymbol{\xi}}U(\boldsymbol{\xi}),\ \
s.t.:\ q(\mathbf{M}|\mathcal{D}) \in \mathcal{P}_{post} (\boldsymbol{\xi})
$


### Infinite SVMs

Infinite SVMs are a Dirichlet process mixture of large-margin kernel machines for multi-way classification.

Learning a single large margin classifier can be too expensive given the increasing complexity of massive training data size, instead by splitting the input data space into subregions, a simpler local large margin classifier can be efficiently learned within each group, flexible non-linear local classifiers that take into account the underlying input data structure.

A technique used to avoid overfitting is to take a mixture of linear SVMs. Each of the linear SVMs have their own linear boundaries. A hidden random variable is used to determine which one of the SVM’s boundaries should be used.

To combine Bayesian nonparametrics and large-margin learning, we can introduce a *Dirichlet Process prior* to automatically set the number of SVMs in mixture model to be used.

**DP mixture of large-margin classifiers**:

Component-wise discriminant function:
$$
F(y, \mathbf{x};z, \mathbf{η}) = η^⊤_z\ f(y, \mathbf{x}) = \sum^{∞}_{i=1}δ_{z,i}η^⊤_i f(y, \mathbf{x})
$$

Overall discriminant function:
$$F(y, \mathbf{x}) = E_{q(\mathbf{η}, \mathbf{z})} [F(y, \mathbf{x};z, \mathbf{η})]
= E_{q(\mathbf{η}, \mathbf{z})} [η_z]^⊤ f(y,\mathbf{x}) 
= \sum^{∞}_{i=1} q(z = i) E_q [η_i]^⊤ f(y, \mathbf{x})
$$

Entropic regularized risk minimization problem:
$$
\min_{q(\mathbf{z},\mathbf{η})} KL(q(\mathbf{z}, \mathbf{η})\| p_0(\mathbf{z}, \mathbf{η})) + C_1 \mathcal{R}(q(\mathbf{z}, \mathbf{η})) $$

$p_0(\mathbf{z}, \mathbf{η}) \sim \mathcal{DP}(\alpha, G_0)$
$\mathcal{R}(q(\mathbf{z}, \mathbf{η})) = \sum_{d}\max_{y}(ℓ_d^{\triangle}(y)+F(y, \mathbf{x}_d)-F(y_d,\mathbf{x}_d))$


**DP mixture of input features:**
$$
\min_{q(\mathbf{z}, \boldsymbol{\gamma}, \mathbf{v})} KL(q(\mathbf{z}, \boldsymbol{\gamma}, \mathbf{v})\| p_0(\mathbf{z}, \boldsymbol{\gamma}, \mathbf{v}|\mathcal{D}))
$$

**Hybrid learning** combining two parts: devise a hybrid objective to learn an optimal posterior distribution $q(\mathbf{z},\mathbf{η}|\mathcal{D})$ for the DP mixture of large-margin classifiers, and to infer $p(\mathbf{z}, \boldsymbol{\gamma}, \mathbf{v} | \mathcal{D})$ for the DP mixture of input features to uncover underlying structures.
$$
\min_{q(\mathbf{z}, \boldsymbol{\eta}, \boldsymbol{\gamma}, \mathbf{v})} f(q(\mathbf{z},\mathbf{η})) + C_2\cdot KL(q(\mathbf{z}, \boldsymbol{\gamma}, \mathbf{v})\| p_0(\mathbf{z}, \boldsymbol{\gamma}, \mathbf{v}|\mathcal{D}))
$$

### Infinite Latent SVMs

Instead of using nonparametric Bayesian prior which indirectly influence the posterior distribution via trade-off with likelihood, it is more *direct* to learn a desirable latent variable model by imposing posterior regularization.

Introduce *Indian Buffet Process prior* for infinite binary feature matrix to allow the models to have an unbounded number of predictive latent features, thus automatically resolve the unknown dimensionality of latent features from data.

**Effective discriminant function** $f$ :
$$
f(y, \mathbf{x};p(\mathbf{Z}, \mathbf{η})) = \mathbb{E}_{p(\mathbf{Z}, \mathbf{η})} [f(y, \mathbf{x}, \mathbf{z}; \mathbf{η})] = \mathbb{E}_{p(\mathbf{Z},\mathbf{η})}[\mathbf{η}^⊤ g(y, \mathbf{x}, \mathbf{z})]
$$

**Posterior constraints with max-margin principle**:
$$
\mathcal{P}^c_{post}(\mathbf{ξ}) = \left\{ \right. p(\mathbf{Z}, \mathbf{η})| 
\begin{aligned}
∀ n ∈ \mathcal{I}_{tr} : 
& f(y_n, x_n; p(\mathbf{Z}, \mathbf{η})) − f(y, x_n;p(\mathbf{Z}, \mathbf{η})) ≥ ℓ(y, y_n)− ξ_n, ∀y \\
& ξ_n ≥ 0 
\end{aligned}
\left. \right\}
$$

**RegBayes problem for iLSVM**:
$$
\begin{aligned} 
& \inf_{q(\mathbf{Z}, \boldsymbol{\eta}, \mathbf{W}),\ \boldsymbol{\xi} } KL({q(\mathbf{Z}, \boldsymbol{\eta}, \mathbf{W})}\| p({\mathbf{Z}, \boldsymbol{\eta}, \mathbf{W}, \mathcal{D}})  + U^c(\boldsymbol{\xi}) \\
& \quad s.t.:\ \ q(\mathbf{Z}, \boldsymbol{\eta}, \mathbf{W}) \in \mathcal{P}^c_{post} (\boldsymbol{\xi})
\end{aligned}
$$

where $p(\mathbf{Z}, \mathbf{η}, \mathbf{W}, \mathcal{D}) = π(\mathbf{η})π(\mathbf{Z})π(\mathbf{W}) \prod^{N}_{n=1} p(\mathbf{x}_n |\mathbf{z}_n , \mathbf{W}, σ_{n_0}^2 )$ is the joint distribution of the model; $π(\mathbf{Z})$ is an IBP prior; and $π(\mathbf{η})$ and $π(\mathbf{W})$ are Gaussian process priors with identity covariance functions.


#### Reference
> - Jun Zhu, Ning Chen, Eric P. Xing, [Infinite SVM: a Dirichlet Process Mixture of Large-margin Kernel Machines][24]
> - Jun Zhu, Ning Chen, Eric P. Xing, [Infinite Latent SVM for Classification and Multi-task Learning][25]
> - Jun Zhu, Ning Chen, Eric P. Xing, [Bayesian Inference with Posterior Regularization and applications to Infinite Latent SVMs][26]


  [1]: http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/dp.pdf
  [2]: https://www.ee.washington.edu/techsite/papers/documents/UWEETR-2010-0006.pdf
  [3]: https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CDAQFjAA&url=http://www.stat.purdue.edu/~rdutta/24.PDF&ei=KWFwU4H1MoL-8QWJuYGgAg&usg=AFQjCNH1723NYwZpyZWJBrhRzuJe78brDw&sig2=pClpfw_FZi0eVPFbLOuceg
  [4]: https://www.cs.princeton.edu/courses/archive/fall07/cos597C/readings/BleiJordan2005.pdf
  [5]: https://www.cs.princeton.edu/courses/archive/fall07/cos597C/readings/TehJordanBealBlei2007.pdf
  [6]: http://mlg.eng.cam.ac.uk/zoubin/papers/ihmm.pdf
  [7]: https://www.cs.princeton.edu/courses/archive/fall07/cos597C/readings/GriffithsGhahramani2006.pdf
  [8]: http://jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf
  [9]: http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS07_ThibauxJ.pdf
  [10]: http://mlg.eng.cam.ac.uk/zoubin/papers/TehGorGha07.pdf
  [11]: https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0CEMQFjAC&url=http://www.seas.upenn.edu/~taskar/pubs/mmmn.pdf&ei=-FNwU5KUOpLt8AXRz4DoAQ&usg=AFQjCNELyH2z_UVq9ZGtxKhMc9iUxkY74g&sig2=PZGexus0sKRe4qKJHeKBJw
  [12]: http://www.di.ens.fr/~slacoste/school/cs281a/M3netReportpdf.pdf
  [13]: http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/dp.pdf
  [14]: https://www.ee.washington.edu/techsite/papers/documents/UWEETR-2010-0006.pdf
  [15]: https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CC4QFjAA&url=https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf&ei=-2VwU9rWKIr4lAXviYGYDg&usg=AFQjCNE45Co9YFJOoovIyW3cEWdcnO5mgQ&sig2=dqlt9i5sLsbyGcQpbNLnwg
  [16]: http://psiexp.ss.uci.edu/research/papers/sciencetopics.pdf
  [17]: http://people.ee.duke.edu/~lcarin/Teh_nips2006.pdf
  [18]: http://papers.nips.cc/paper/3328-supervised-topic-models.pdf
  [19]: http://www.ml-thu.net/~jun/pub/medlda_icml09.pdf
  [20]: https://www.cs.princeton.edu/courses/archive/fall07/cos597C/readings/GriffithsGhahramani2006.pdf
  [21]: http://202.113.18.194/cache/5/03/www.ml-thu.net/362637945ccc27ee97ae5c2209fb239f/MedLDA_jmlr.pdf
  [22]: http://jmlr.org/proceedings/papers/v28/zhu13.pdf
  [23]: http://arxiv.org/pdf/1310.2816v1.pdf
  [24]: http://books.nips.cc/papers/files/nips24/NIPS2011_0931.pdf
  [25]: http://arxiv.org/abs/1210.1766
  [26]: http://arxiv.org/abs/1210.1766
